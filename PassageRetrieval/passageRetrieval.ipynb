{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passage Retrievers\n",
    "\n",
    "- TF-IDF\n",
    "- BM25\n",
    "- Dense Passage Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# SQuAD Example\n",
    "SQuAD format contains question, context and answers\n",
    "\n",
    "Extractive QA:\n",
    "Open book extractive model:\n",
    "- Accepts question\n",
    "- Converts sentence to query vector\n",
    "- Compares vector with database of document or passage vectors\n",
    "- Returns top k most similar context vectors\n",
    "- Contexts are sent to reader\n",
    "- Reader 'reads' context vectors and selects 'answer' passage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PreProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Sentence transformers from sbert.net\n",
    "<br>It is a pretrained model for QA, we want a model that uses cosine similarity so we take one which is trained for that use\n",
    "<br>We use the fastest but necessarily the best performing for this demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 5.27kB [00:00, 1.13MB/s]                   \n",
      "Downloading metadata: 2.36kB [00:00, 785kB/s]                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset squad/plain_text (download: 33.51 MiB, generated: 85.63 MiB, post-processed: Unknown size, total: 119.14 MiB) to /Users/johncmcc/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 30.3MB [00:00, 65.9MB/s]/2 [00:00<?, ?it/s]\n",
      "Downloading data: 4.85MB [00:00, 67.5MB/s]                   .35it/s]\n",
      "Downloading data files: 100%|██████████| 2/2 [00:01<00:00,  1.82it/s]\n",
      "Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 729.38it/s]\n",
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset squad downloaded and prepared to /Users/johncmcc/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 10570\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "qa = datasets.load_dataset('squad', split='validation')\n",
    "qa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '56be4db0acb8001400a502ec',\n",
       " 'title': 'Super_Bowl_50',\n",
       " 'context': 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.',\n",
       " 'question': 'Which NFL team represented the AFC at Super Bowl 50?',\n",
       " 'answers': {'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'],\n",
       "  'answer_start': [177, 177, 177]}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 16.86ba/s]\n"
     ]
    }
   ],
   "source": [
    "from sympy import false\n",
    "\n",
    "\n",
    "unique_contexts = []\n",
    "unique_ids = []\n",
    "\n",
    "# make list of ids that represent only the first instance of each context\n",
    "\n",
    "for row in qa:\n",
    "    if row['context'] not in unique_contexts:\n",
    "        unique_contexts.append(row['context'])\n",
    "        unique_ids.append(row['id'])\n",
    "\n",
    "# filtering samples that aren't included in unique ids\n",
    "qa = qa.filter(lambda x: True if x['id'] in unique_ids else False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create context vectors with the retriever model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 737/737 [00:00<00:00, 347kB/s]\n",
      "Downloading: 100%|██████████| 190/190 [00:00<00:00, 86.3kB/s]\n",
      "Downloading: 100%|██████████| 11.3k/11.3k [00:00<00:00, 4.42MB/s]\n",
      "Downloading: 100%|██████████| 612/612 [00:00<00:00, 272kB/s]\n",
      "Downloading: 100%|██████████| 116/116 [00:00<00:00, 44.0kB/s]\n",
      "Downloading: 100%|██████████| 25.5k/25.5k [00:00<00:00, 145kB/s] \n",
      "Downloading: 100%|██████████| 90.9M/90.9M [00:02<00:00, 45.3MB/s]\n",
      "Downloading: 100%|██████████| 53.0/53.0 [00:00<00:00, 28.8kB/s]\n",
      "Downloading: 100%|██████████| 112/112 [00:00<00:00, 39.2kB/s]\n",
      "Downloading: 100%|██████████| 466k/466k [00:00<00:00, 540kB/s]  \n",
      "Downloading: 100%|██████████| 383/383 [00:00<00:00, 204kB/s]\n",
      "Downloading: 100%|██████████| 13.8k/13.8k [00:00<00:00, 80.8kB/s]\n",
      "Downloading: 100%|██████████| 232k/232k [00:00<00:00, 336kB/s]  \n",
      "Downloading: 100%|██████████| 349/349 [00:00<00:00, 164kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the context vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a long time places contexts nto new feature called encoding\n",
    "qa = qa.map(lambda x:{\n",
    "    'encoding': model.encode(x['context']).tolist()\n",
    "}, batched=True, batch_size=32)\n",
    "qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.encode('hello world').tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the vector database and indexing context vectors.\n",
    "\n",
    "The retriever will be referring to the vector to the database to retrieve context\n",
    "\n",
    "Using FAISS or using Pinecone\n",
    "Using Pinecone in this example, I will need to get an API key from app.pinecone.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"22636ca9-6458-408b-8e48-8346f2a76f2f\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "pinecone.init(API_KEY, environment='us-west1-gcp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/np/2p8fxgg962q5jcpzzkjrmb3r0000gn/T/ipykernel_69969/3660307942.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hello world'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpinecone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'qa-index'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/lib/python3.8/site-packages/pinecone/manage.py\u001b[0m in \u001b[0;36mcreate_index\u001b[0;34m(name, dimension, timeout, index_type, metric, replicas, shards, pods, pod_type, index_config, metadata_config)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_ready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mis_ready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Pass in an index name and the dimensionality of the index\n",
    "# the dimension should match the length of the encoding that the model , all our vectors will have that dimension\n",
    "dim = len(model.encode('hello world').tolist())\n",
    "pinecone.create_index('qa-index', dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Populate the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to qa index\n",
    "index = pinecone.Index('qa-index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Upsert' Upload and insert vectors into index\n",
    "<br>Using a batch upload process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:00<00:00, 56371.45it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    " \n",
    "# When upserting to pinecone we want a tuple with id and the vector | metadata optional\n",
    "upserts = [(v['id'], v['title']) for v in qa]\n",
    "\n",
    "# progress bar\n",
    "l_ups = len(upserts)\n",
    "\n",
    "for i in tqdm(range(0, l_ups, 50)):\n",
    "    i_end = i + 50\n",
    "    if i_end > l_ups:\n",
    "        i_end = l_ups\n",
    "    index.upsert(vectors=upserts[i:i_end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2050, 2067)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i, i_end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to encode our queries in this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\n",
    "model.encode().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression Analysis\n",
    "    Linear regression\n",
    "    Multiple regression analysis\n",
    "    Dummy Variables\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('3.8.12')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a678f92b11b0cdfffc6334f5c4b8d213737878ded07f1a6ebc7b8f8950f72291"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
